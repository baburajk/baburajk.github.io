<!DOCTYPE html>
<html lang="en">
<head>  
	<meta charset="utf-8">
	<title>Network Namespaces</title>
 
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="path/to/image.jpg">
 
<!-- build:css -->
<link rel="stylesheet" href="css/libs/bootstrap.min.css">
<link rel="stylesheet" href="css/libs/font-awesome.min.css">
<link rel="stylesheet" href="css/libs/animate.min.css">
<link rel="stylesheet" href="css/libs/slick.css">
<link rel="stylesheet" href="css/libs/magnific-popup.css">

<link rel="stylesheet" href="css/main.css">
<link rel="stylesheet" href="css/media.css">
<!-- endbuild -->
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#000">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#000">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#000">
	<!-- <style>body { opacity:s 0; overflow-x: hidden; } html { background-color: #fff; }</style> -->
</head>
<body data-spy="scroll" data-target=".navbar" data-offset="50" class="loaded" link="blue">

	
<div class="site-content">
	<!-- Naviigation -->
	<div class="navbar-wrap">
		<div class="navbar">
			<div class="container">
				<div class="row">
					<div class="col-md-12">
						<nav class="navbar-menu">
							<div class="navbar-header">
								<button class="collapsed navbar-toggle" type="button" data-toggle="collapse" data-target=".bs-example-js-navbar-scrollspy">
									<span class="icon-bar"></span>
									<span class="icon-bar"></span>
									<span class="icon-bar"></span>
								</button>
							</div>
							 
							<div class="navbar-full">
								<div class="collapse bs-example-js-navbar-scrollspy">
									<ul class="nav navbar-nav">
										<li style="display: none;"><a></a></li>
										<li><a href="http://www.linkedin.com/in/baburajkallarakkal">About Me</a></li>
										<li><a href="http://padmavyuha.blogspot.com/">Blog</a></li>
										<li><a href="https://sourceforge.net/projects/oraclerman/">Projects</a></li>
										<li><a href="https://hub.docker.com/u/baburaj/">Docker</a></li>
										<li><a href="mailto:raj.anju@gmail.com">Contact Me</a></li>
										<li>
										</li>
									</ul>
							
								</div>
							</div>
						</nav>
					</div>
				</div>
			</div>
		</div>
	</div>
	<!-- Naviigation -->
	

<!-- Header -->
	<div class="menu-sticky"></div>
 
	<!-- Screen-one -->
	<div class="screen-one">
		<div class="container">
			<div class="row">
				<div class="col-md-12">
					<div class="screen-one-title">

						<h2> Network Namespace Internals to under Docker Interpod Communication and analysis of IPTable Rules   </h2>
					</div>
				</div>
			</div>
		</div>
	 
		<div class="container">
 
<!-- container -->		
		<div class="screen-one-item-container">
                            <h3>  Network Diagram </h3>
                            <img src="./imgs/namespace-v2.svg" width="800" height="800"  alt="Network Configuration">   
							<p>  </p>
							<p> </p>

<pre>

</pre>		

		</div>
<!-- container -->		


<!-- container -->		
<div class="screen-one-item-container">
	<h3>  Goal - Simulate docker interpod communication using network namespaces. </h3>
	<p>  There are lot of nice articles explaining how the Network Namespaces works and how Docker Containerization makes use of the network namespaces   
        for inter pod communications. To get a practical sense of how this works, two network namespaces was created on a KVM Guest VM. The VM used already has a docker daemon
        
        Running and instead of cleaning those up, I decided to use the same to see if those IP Table rules created by docker would create any issue, if so how and also to learn and use those rules
        as a reference point while setting these up.
        
        Logical diagram represents a quick high level over view of the overall configuration.
        
        A quick recap from the web, a network namespace is logically another copy of the network stack, with its  own routes, firewall rules, and network devices. y default a process inherits its network namespace from its parent. Initially all the processes share the same default network namespace from the init process.
    </p>
	<p> </p>

<pre>

</pre>		

</div>
<!-- container -->		


<!-- container -->		
<div class="screen-one-item-container">
	<h3>  Default Configurations. </h3>
	<p> What does the defaults looks like before any changes were made! The default route to 192.168.122.0/24 is 
        the one for the Guest VM to communicate to the hypervisor network and 172.17.0.0/16  is the one defined for docker daemon. Docker daemon , corresponding routes etc. was created for all the guest VM’s and for the purpose of this experiment , though I really don’t need them - decided to leave it out for two reasons. a) I can use the “iptable" rules created by docker itself as a reference b) If these rules by any reasons interfere that will result in more of a learning experience and go bit more deeper..
    </p>
	<p> </p>

<pre>


    [root@rhel8b ~]# ip route show
    default via 192.168.122.1 dev enp1s0 proto dhcp metric 100
    172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
    192.168.122.0/24 dev enp1s0 proto kernel scope link src 192.168.122.130 metric 100
    
    [root@rhel8b ~]# ip addr show
    1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
        inet 127.0.0.1/8 scope host lo
           valid_lft forever preferred_lft forever
        inet6 ::1/128 scope host
           valid_lft forever preferred_lft forever
    2: enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        link/ether 52:54:00:01:1a:b3 brd ff:ff:ff:ff:ff:ff
        inet 192.168.122.130/24 brd 192.168.122.255 scope global dynamic noprefixroute enp1s0
           valid_lft 3284sec preferred_lft 3284sec
        inet6 2001:db8:ca2:3:1::37/128 scope global dynamic noprefixroute
           valid_lft 2888sec preferred_lft 2888sec
        inet6 fe80::b6c6:db5e:4ae7:956a/64 scope link noprefixroute
           valid_lft forever preferred_lft forever
    3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default
        link/ether 02:42:67:e9:9c:2a brd ff:ff:ff:ff:ff:ff
        inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
           valid_lft forever preferred_lft forever
    
</pre>		

</div>
<!-- container -->		



<!-- container -->		
<div class="screen-one-item-container">
	<h3>  Namespaces and VETH Pairs </h3>
	<p> The veth pairs created would be in the root namespace by default and will not be visible on the cnetns0 and cnetns1 namespaces.

        Once we have the veth pairs , move the peer end to the respective namespaces. An easy way to identify the peer name is by looking at the interface ID. 

        How do we know the peer name for a veth network interface? 
        The interface ID of veth0 is “5” and the peer name for that veth is ceth0@if5 . 
        So it’s veth&lt;Interface ID&gt; == ceth0@if&lt;Interface ID&gt;
    </p>
	<p> </p>

<pre>

    [root@rhel8b netns]# ip link set dev ceth0 netns cnetns0
    [root@rhel8b netns]# ip link set dev ceth1 netns cnetns1
    
    
    [root@rhel8b netns]#  ip netns exec cnetns0 ip link show
    1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000
        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    4: ceth0@if5: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
        link/ether 3a:9c:07:b7:10:bd brd ff:ff:ff:ff:ff:ff link-netnsid 0
    
    [root@rhel8b netns]#  ip netns exec cnetns1 ip link show
    1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000
        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    6: ceth1@if7: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
        link/ether f6:0e:f5:ee:d1:7e brd ff:ff:ff:ff:ff:ff link-netnsid 0

</pre>		

</div>
<!-- container -->		


<!-- container -->		
<div class="screen-one-item-container">
	<h3>  Routing and IP Address Configuration. </h3>
	<p> At this point neither of these interfaces - ceth0 or ceth1 can communicate with each other since no 
        IP addresses has been assigned, so login to each namespaces and assign the IP addresses. Since 172.16.0.0/16 is already assigned to the docker network and as I’ve not shutdown this network, decided to use a separate network space - 172.20.0.0/16  for this purpose. 

    </p>
	<p> </p>

<pre>

    [root@rhel8b netns]# nsenter --net=/var/run/netns/cnetns0 bash
    
    [root@rhel8b netns]# ip link show
    1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000
        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    4: ceth0@if5: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
        link/ether 3a:9c:07:b7:10:bd brd ff:ff:ff:ff:ff:ff link-netnsid 0

    [root@rhel8b netns]#  ip addr show
    1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1000
        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    4: ceth0@if5: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000
        link/ether 3a:9c:07:b7:10:bd brd ff:ff:ff:ff:ff:ff link-netnsid 0
        
    [root@rhel8b netns]# ip addr add 172.20.0.50/16 dev ceth0

    [root@rhel8b netns]# ip addr show
    1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1000
        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    4: ceth0@if5: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000
        link/ether 3a:9c:07:b7:10:bd brd ff:ff:ff:ff:ff:ff link-netnsid 0
        inet 172.20.0.50/16 scope global ceth0
           valid_lft forever preferred_lft forever
   
    [root@rhel8b netns]# ip route show
    
    
</pre>		

</div>
<!-- container -->		


<!-- container -->		
<div class="screen-one-item-container">
	<h3>  Routing and IP Address Configuration - Contd. </h3>
	<p> At this point IP address is added, but no routes has been set and this is because the physical links are down still and has not been brought up yet.

    </p>
	<p> </p>

<pre>
    [root@rhel8b netns]# ip link set ceth0 up
    [root@rhel8b netns]# ip link set lo up
    
    [root@rhel8b netns]# ip link show
    1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    4: ceth0@if5: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state LOWERLAYERDOWN mode DEFAULT group default qlen 1000
        link/ether 3a:9c:07:b7:10:bd brd ff:ff:ff:ff:ff:ff link-netnsid 0
    
    [root@rhel8b netns]# ip addr show
    1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
        inet 127.0.0.1/8 scope host lo
           valid_lft forever preferred_lft forever
        inet6 ::1/128 scope host
           valid_lft forever preferred_lft forever
    4: ceth0@if5: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state LOWERLAYERDOWN group default qlen 1000
        link/ether 3a:9c:07:b7:10:bd brd ff:ff:ff:ff:ff:ff link-netnsid 0
        inet 172.20.0.50/16 scope global ceth0
           valid_lft forever preferred_lft forever

           Note: The state of ceth0 is showing “LOWERLAYERDOWN”, though the interface itself is "<UP>" since the other end of the veth pair was only
            added and has not been brought up. Also by bringing this up the routes are also plugged into the 
            routing table which in the previous output was empty.


    [root@rhel8b netns]# ip route show
    172.20.0.0/16 dev ceth0 proto kernel scope link src 172.20.0.50 linkdown

    #Open another terminal and run similar commands on the cnetns1 to complete the configuration.

        nsenter --net=/var/run/netns/cnetns1 bash
        ip link set lo up
        ip link set ceth1 up
        ip addr add 172.20.0.60/16 dev ceth1
        ip add show


</pre>		

</div>
<!-- container -->		


<!-- container -->		
<div class="screen-one-item-container">
	<h3> Root Namespace - VETH setup.   </h3>
	<p> The interface links on the root namespace is still down which needs to be brought up. 
    </p>
	<p> </p>

<pre>

    [root@rhel8b ~]# ip link show veth0
    5: veth0@if4: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
        link/ether 32:e4:7c:d5:bd:8f brd ff:ff:ff:ff:ff:ff link-netns cnetns0
    
    [root@rhel8b ~]# ip link show veth1
    7: veth1@if6: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
        link/ether ea:c9:3d:5b:64:59 brd ff:ff:ff:ff:ff:ff link-netns cnetns1 
    
    
    ip link set dev veth0 up
    ip link set dev veth1 up
    
    
    [root@rhel8b ~]# ip link show veth0
    5: veth0@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
        link/ether 32:e4:7c:d5:bd:8f brd ff:ff:ff:ff:ff:ff link-netns cnetns0
    [root@rhel8b ~]#
    [root@rhel8b ~]# ip link show veth1
    7: veth1@if6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
        link/ether ea:c9:3d:5b:64:59 brd ff:ff:ff:ff:ff:ff link-netns cnetns1
    
    After the peer has been brought up, the interface status of ceth0 and ceth1 within the network namespaces are 
    changed from “LOWERLAYERDOWN” to  “LOWER_UP” as seen below.
    
    [root@rhel8b-cnetns0 ~ # ip link show ceth0
    4: ceth0@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
        link/ether 3a:9c:07:b7:10:bd brd ff:ff:ff:ff:ff:ff link-netnsid 0
    
    [root@rhel8b-cnetns1 ~ # ip link show ceth1
    6: ceth1@if7: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
        link/ether f6:0e:f5:ee:d1:7e brd ff:ff:ff:ff:ff:ff link-netnsid 0
    
</pre>		

</div>
<!-- container -->		



<!-- container -->		
<div class="screen-one-item-container">
	<h3> Reachability Tests. </h3>
	<p> The basic setups are done, it's about time to do the simple reachability tests. </p>
	<p> 
        # Provides the inode number associated with the process.
        [root@rhel8b ~]# lsns -o NS,PATH,USER,NETNSID | grep $(echo $$)
        4026532414 /proc/2093/ns/net root   unassigned
        
        The basic 3 use-cases that I want to test it out are as follows.
     
        a) Can one cnetns0/IP reach the IP defined on cnetns1
        b) Can any of these IP’s reach internet.
        c) Can the VM reach the IP’s defined on these NS - cnetns0/1
    </p>

<pre>



</pre>		

</div>
<!-- container -->		


<!-- container -->		
<div class="screen-one-item-container">
	<h3>  SUBTITLE </h3>
	<p> </p>
	<p> </p>

<pre>
    Since there are two terminals/sessions opened to two different network NS, to identify the sessions, change
    the PS1 (prompt) so it's easier.

    [root@rhel8b ~]# ip netns identify $(echo $$)
    cnetns0
    
    Some additional commands for reference.
    
    # Sets the bash prompt
    export PS1="[\u@\h-$(ip netns identify $(echo $$)) \w # "

</pre>		

</div>
<!-- container -->		


<!-- container -->		
<div class="screen-one-item-container">
	<h3>  SUBTITLE </h3>
	<p> </p>
	<p> </p>

<pre>
CODE BLOCK
</pre>		

</div>
<!-- container -->		


<!-- container -->		
<div class="screen-one-item-container">
	<h3>  SUBTITLE </h3>
	<p> </p>
	<p> </p>

<pre>
CODE BLOCK
</pre>		

</div>
<!-- container -->		


<!-- container -->		
<div class="screen-one-item-container">
	<h3>  SUBTITLE </h3>
	<p> </p>
	<p> </p>

<pre>
CODE BLOCK
</pre>		

</div>
<!-- container -->		



	

</body>
</html>
