<!DOCTYPE html>
<html lang="en">
<head>  
	<meta charset="utf-8">
	<title>Network Namespaces</title>
 
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="path/to/image.jpg">
 
<!-- build:css -->
<link rel="stylesheet" href="css/libs/bootstrap.min.css">
<link rel="stylesheet" href="css/libs/font-awesome.min.css">
<link rel="stylesheet" href="css/libs/animate.min.css">
<link rel="stylesheet" href="css/libs/slick.css">
<link rel="stylesheet" href="css/libs/magnific-popup.css">

<link rel="stylesheet" href="css/main.css">
<link rel="stylesheet" href="css/media.css">
<!-- endbuild -->
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#000">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#000">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#000">
	<!-- <style>body { opacity:s 0; overflow-x: hidden; } html { background-color: #fff; }</style> -->
</head>
<body data-spy="scroll" data-target=".navbar" data-offset="50" class="loaded" link="blue">

	
<div class="site-content">
	<!-- Naviigation -->
	<div class="navbar-wrap">
		<div class="navbar">
			<div class="container">
				<div class="row">
					<div class="col-md-12">
						<nav class="navbar-menu">
							<div class="navbar-header">
								<button class="collapsed navbar-toggle" type="button" data-toggle="collapse" data-target=".bs-example-js-navbar-scrollspy">
									<span class="icon-bar"></span>
									<span class="icon-bar"></span>
									<span class="icon-bar"></span>
								</button>
							</div>
							 
							<div class="navbar-full">
								<div class="collapse bs-example-js-navbar-scrollspy">
									<ul class="nav navbar-nav">
										<li style="display: none;"><a></a></li>
										<li><a href="http://www.linkedin.com/in/baburajkallarakkal">About Me</a></li>
										<li><a href="http://padmavyuha.blogspot.com/">Blog</a></li>
										<li><a href="https://sourceforge.net/projects/oraclerman/">Projects</a></li>
										<li><a href="https://hub.docker.com/u/baburaj/">Docker</a></li>
										<li><a href="mailto:raj.anju@gmail.com">Contact Me</a></li>
										<li>
										</li>
									</ul>
							
								</div>
							</div>
						</nav>
					</div>
				</div>
			</div>
		</div>
	</div>
	<!-- Naviigation -->
	

<!-- Header -->
	<div class="menu-sticky"></div>
 
	<!-- Screen-one -->
	<div class="screen-one">
		<div class="container">
			<div class="row">
				<div class="col-md-12">
					<div class="screen-one-title">

						<h2> Network Namespace Internals to under Docker Interpod Communication and analysis of IPTable Rules   </h2>
					</div>
				</div>
			</div>
		</div>
	 
		<div class="container">
 
<!-- container -->		
		<div class="screen-one-item-container">
                            <h3>  </h3>
                            <img src="./imgs/namespace-v3.svg" width="800" height="800"  alt="Network Configuration">   
							<p>  </p>
							<p> </p>
 
		</div>
<!-- container -->		


<!-- container -->		
<div class="screen-one-item-container">
	<h3> Simulate docker interpod communication using network namespaces. </h3>
	<p> A quick recap from the web, a network namespace is logically another copy of the network stack, with its  own routes, firewall rules, and network devices. 
        There are lot of nice articles explaining how the Network Namespaces works and how Docker Containerization makes use of the network namespaces   
        for inter pod communications. For the purpose of the experiment two network namespaces was created on a KVM Guest VM manually to understand
        the routing aspects and as well the IP Table rules required to enable the communication between the POD's, egress from POD to outside world and also
        ingress to the POD's.

      
        
    </p>
	<p> </p>

<pre>

</pre>		

</div>
<!-- container -->		


<!-- container -->		
<div class="screen-one-item-container">
	<h3>  Default Configurations. </h3>
	<p> What does the defaults looks like before any changes were made! The default route to 192.168.122.0/24 is 
        the one for the Guest VM to communicate to the hypervisor network and 172.17.0.0/16  is the one defined for docker daemon. Docker daemon , corresponding routes etc. was created for all the guest VM’s and for the purpose of this experiment , though I really don’t need them - decided to leave it out for two reasons. a) I can use the “iptable" rules created by docker itself as a reference b) If these rules by any reasons interfere that will result in more of a learning experience and go bit more deeper..
    </p>
	<p> </p>

<pre>


    [root@rhel8b ~]# ip route show
    default via 192.168.122.1 dev enp1s0 proto dhcp metric 100
    172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
    192.168.122.0/24 dev enp1s0 proto kernel scope link src 192.168.122.130 metric 100
    
    [root@rhel8b ~]# ip addr show
    1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
        inet 127.0.0.1/8 scope host lo
           valid_lft forever preferred_lft forever
        inet6 ::1/128 scope host
           valid_lft forever preferred_lft forever
    2: enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        link/ether 52:54:00:01:1a:b3 brd ff:ff:ff:ff:ff:ff
        inet 192.168.122.130/24 brd 192.168.122.255 scope global dynamic noprefixroute enp1s0
           valid_lft 3284sec preferred_lft 3284sec
        inet6 2001:db8:ca2:3:1::37/128 scope global dynamic noprefixroute
           valid_lft 2888sec preferred_lft 2888sec
        inet6 fe80::b6c6:db5e:4ae7:956a/64 scope link noprefixroute
           valid_lft forever preferred_lft forever
    3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default
        link/ether 02:42:67:e9:9c:2a brd ff:ff:ff:ff:ff:ff
        inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
           valid_lft forever preferred_lft forever
    
</pre>		

</div>
<!-- container -->		



<!-- container -->		
<div class="screen-one-item-container">
	<h3>  Namespaces and VETH Pairs </h3>
	<p> The veth pairs created would be in the root namespace by default and will not be visible on the cnetns0 and cnetns1 namespaces.

        Once we have the veth pairs , move the peer end to the respective namespaces. An easy way to identify the peer name is by looking at the interface ID. 

        How do we know the peer name for a veth network interface? 
        The interface ID of veth0 is “5” and the peer name for that veth is ceth0@if5 . 
        So it’s veth&lt;Interface ID&gt; == ceth0@if&lt;Interface ID&gt;
    </p>
	<p> </p>

<pre>

    [root@rhel8b netns]# ip link set dev ceth0 netns cnetns0
    [root@rhel8b netns]# ip link set dev ceth1 netns cnetns1
    
    
    [root@rhel8b netns]#  ip netns exec cnetns0 ip link show
    1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000
        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    4: ceth0@if5: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
        link/ether 3a:9c:07:b7:10:bd brd ff:ff:ff:ff:ff:ff link-netnsid 0
    
    [root@rhel8b netns]#  ip netns exec cnetns1 ip link show
    1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000
        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    6: ceth1@if7: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
        link/ether f6:0e:f5:ee:d1:7e brd ff:ff:ff:ff:ff:ff link-netnsid 0

</pre>		

</div>
<!-- container -->		


<!-- container -->		
<div class="screen-one-item-container">
	<h3>  Routing and IP Address Configuration. </h3>
	<p> At this point neither of these interfaces - ceth0 or ceth1 can communicate with each other since no 
        IP addresses has been assigned, so login to each namespaces and assign the IP addresses. Since 172.16.0.0/16 is already assigned to the docker network and as I’ve not shutdown this network, decided to use a separate network space - 172.20.0.0/16  for this purpose. 

    </p>
	<p> </p>

<pre>

    [root@rhel8b netns]# nsenter --net=/var/run/netns/cnetns0 bash
    
    [root@rhel8b netns]# ip link show
    1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000
        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    4: ceth0@if5: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
        link/ether 3a:9c:07:b7:10:bd brd ff:ff:ff:ff:ff:ff link-netnsid 0

    [root@rhel8b netns]#  ip addr show
    1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1000
        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    4: ceth0@if5: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000
        link/ether 3a:9c:07:b7:10:bd brd ff:ff:ff:ff:ff:ff link-netnsid 0
        
    [root@rhel8b netns]# ip addr add 172.20.0.50/16 dev ceth0

    [root@rhel8b netns]# ip addr show
    1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1000
        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    4: ceth0@if5: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000
        link/ether 3a:9c:07:b7:10:bd brd ff:ff:ff:ff:ff:ff link-netnsid 0
        inet 172.20.0.50/16 scope global ceth0
           valid_lft forever preferred_lft forever
   
    [root@rhel8b netns]# ip route show
    
    
</pre>		

</div>
<!-- container -->		


<!-- container -->		
<div class="screen-one-item-container">
	<h3>  Routing and IP Address Configuration - Contd. </h3>
	<p> At this point IP address is added, but no routes has been set and this is because the physical links are down still and has not been brought up yet.

    </p>
	<p> </p>

<pre>
    [root@rhel8b netns]# ip link set ceth0 up
    [root@rhel8b netns]# ip link set lo up
    
    [root@rhel8b netns]# ip link show
    1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    4: ceth0@if5: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state LOWERLAYERDOWN mode DEFAULT group default qlen 1000
        link/ether 3a:9c:07:b7:10:bd brd ff:ff:ff:ff:ff:ff link-netnsid 0
    
    [root@rhel8b netns]# ip addr show
    1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
        inet 127.0.0.1/8 scope host lo
           valid_lft forever preferred_lft forever
        inet6 ::1/128 scope host
           valid_lft forever preferred_lft forever
    4: ceth0@if5: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state LOWERLAYERDOWN group default qlen 1000
        link/ether 3a:9c:07:b7:10:bd brd ff:ff:ff:ff:ff:ff link-netnsid 0
        inet 172.20.0.50/16 scope global ceth0
           valid_lft forever preferred_lft forever

           Note: The state of ceth0 is showing “LOWERLAYERDOWN”, though the interface itself is "<UP>" since the other end of the veth pair was only
            added and has not been brought up. Also by bringing this up the routes are also plugged into the 
            routing table which in the previous output was empty.


    [root@rhel8b netns]# ip route show
    172.20.0.0/16 dev ceth0 proto kernel scope link src 172.20.0.50 linkdown

    #Open another terminal and run similar commands on the cnetns1 to complete the configuration.

        nsenter --net=/var/run/netns/cnetns1 bash
        ip link set lo up
        ip link set ceth1 up
        ip addr add 172.20.0.60/16 dev ceth1
        ip add show


</pre>		

</div>
<!-- container -->		


<!-- container -->		
<div class="screen-one-item-container">
	<h3> Root Namespace - VETH setup.   </h3>
	<p> The interface links on the root namespace is still down which needs to be brought up. 
    </p>
	<p> </p>

<pre>

    [root@rhel8b ~]# ip link show veth0
    5: veth0@if4: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
        link/ether 32:e4:7c:d5:bd:8f brd ff:ff:ff:ff:ff:ff link-netns cnetns0
    
    [root@rhel8b ~]# ip link show veth1
    7: veth1@if6: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
        link/ether ea:c9:3d:5b:64:59 brd ff:ff:ff:ff:ff:ff link-netns cnetns1 
    
    
    ip link set dev veth0 up
    ip link set dev veth1 up
    
    
    [root@rhel8b ~]# ip link show veth0
    5: veth0@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
        link/ether 32:e4:7c:d5:bd:8f brd ff:ff:ff:ff:ff:ff link-netns cnetns0
    [root@rhel8b ~]#
    [root@rhel8b ~]# ip link show veth1
    7: veth1@if6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
        link/ether ea:c9:3d:5b:64:59 brd ff:ff:ff:ff:ff:ff link-netns cnetns1
    
    After the peer has been brought up, the interface status of ceth0 and ceth1 within the network namespaces are 
    changed from “LOWERLAYERDOWN” to  “LOWER_UP” as seen below.
    
    [root@rhel8b-cnetns0 ~ # ip link show ceth0
    4: ceth0@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
        link/ether 3a:9c:07:b7:10:bd brd ff:ff:ff:ff:ff:ff link-netnsid 0
    
    [root@rhel8b-cnetns1 ~ # ip link show ceth1
    6: ceth1@if7: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
        link/ether f6:0e:f5:ee:d1:7e brd ff:ff:ff:ff:ff:ff link-netnsid 0
    

    [root@rhel8b ~]# ip route show
    default via 192.168.122.1 dev enp1s0 proto dhcp metric 100
    172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
    192.168.122.0/24 dev enp1s0 proto kernel scope link src 192.168.122.130 metric 100

    [root@rhel8b ~]# ip addr add 172.20.0.5/16 dev veth0
    [root@rhel8b ~]# ip addr add 172.20.0.6/16 dev veth1

    [root@rhel8b ~]# ip route show
    default via 192.168.122.1 dev enp1s0 proto dhcp metric 100
    172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
    172.20.0.0/16 dev veth0 proto kernel scope link src 172.20.0.5
    172.20.0.0/16 dev veth1 proto kernel scope link src 172.20.0.6
    192.168.122.0/24 dev enp1s0 proto kernel scope link src 192.168.122.130 metric 100
 

</pre>		

</div>
<!-- container -->		



<!-- container -->		
<div class="screen-one-item-container">
	<h3> Reachability Tests. </h3>
	<p> The basic setups are done, it's about time to do the simple reachability tests. </p>
	<p> 
     
    </p>

<pre>

    # Provides the inode number associated with the process.
    [root@rhel8b ~]# lsns -o NS,PATH,USER,NETNSID | grep $(echo $$)
    4026532414 /proc/2093/ns/net root   unassigned
    
    The basic 3 use-cases that I want to test it out are as follows.
 
    a) Can one cnetns0/IP reach the IP defined on cnetns1
    b) Can any of these IP’s reach internet.
    c) Can the VM reach the IP’s defined on these NS - cnetns0/1

    #Additional notes
    Since there are two terminals/sessions opened to two different network NS, to identify the sessions, change
    the PS1 (prompt) so it's easier.

    [root@rhel8b ~]# ip netns identify $(echo $$)
    cnetns0
    
    Some additional commands for reference.
    
    # Sets the bash prompt
    export PS1="[\u@\h-$(ip netns identify $(echo $$)) \w # "


</pre>		

</div>
<!-- container -->		


<!-- container -->		
<div class="screen-one-item-container">
	<h3> Reachability Tests - Contd. </h3>
	<p> </p>
	<p> </p>

<pre>
 

    From rootns to cnetns0 and cnetns1

    [root@rhel8b ~]# ping -c 1 172.20.0.50
    PING 172.20.0.50 (172.20.0.50) 56(84) bytes of data.
    64 bytes from 172.20.0.50: icmp_seq=1 ttl=64 time=0.055 ms
    
    --- 172.20.0.50 ping statistics ---
    1 packets transmitted, 1 received, 0% packet loss, time 0ms
    rtt min/avg/max/mdev = 0.055/0.055/0.055/0.000 ms
    
    [root@rhel8b ~]# ping -c 1 172.20.0.60
    PING 172.20.0.60 (172.20.0.60) 56(84) bytes of data.
    From 172.20.0.5 icmp_seq=1 Destination Host Unreachable
    
    --- 172.20.0.60 ping statistics ---
    1 packets transmitted, 0 received, +1 errors, 100% packet loss, time 0ms
    
    From cnetns0 to rootns and cnetns1
    
    [root@rhel8b-cnetns0 ~ # ping -c 1 172.20.0.5
    PING 172.20.0.5 (172.20.0.5) 56(84) bytes of data.
    64 bytes from 172.20.0.5: icmp_seq=1 ttl=64 time=0.060 ms
    
    --- 172.20.0.5 ping statistics ---
    1 packets transmitted, 1 received, 0% packet loss, time 0ms
    rtt min/avg/max/mdev = 0.060/0.060/0.060/0.000 ms
    
    [root@rhel8b-cnetns0 ~ # ping -c1 172.20.0.60
    PING 172.20.0.60 (172.20.0.60) 56(84) bytes of data.
    From 172.20.0.50 icmp_seq=1 Destination Host Unreachable
    
    --- 172.20.0.60 ping statistics ---
    1 packets transmitted, 0 received, +1 errors, 100% packet loss, time 0ms
    
    From cnetns1 to rootns and cnetns0 
    
    [root@rhel8b-cnetns1 ~ # ping -c 1 172.20.0.6
    PING 172.20.0.6 (172.20.0.6) 56(84) bytes of data.
    From 172.20.0.60 icmp_seq=1 Destination Host Unreachable
    
    --- 172.20.0.6 ping statistics ---
    1 packets transmitted, 0 received, +1 errors, 100% packet loss, time 0ms
    
    [root@rhel8b-cnetns1 ~ # ping -c 1 172.20.0.50
    PING 172.20.0.50 (172.20.0.50) 56(84) bytes of data.
    From 172.20.0.60 icmp_seq=1 Destination Host Unreachable
    
    --- 172.20.0.50 ping statistics ---
    1 packets transmitted, 0 received, +1 errors, 100% packet loss, time 0ms
    
    So the routes are good from rootns and cnetns0  because of which they are reachable but the pings from cnetns1 resulted in 
    100% packet loss!
    

</pre>		

</div>
<!-- container -->		


<!-- container -->		
<div class="screen-one-item-container">
	<h3>  Static routes and precedence! </h3>
	<p> Why would centns0 fail to communicate between either of these namespaces and what are the things
        that can be looked at? So if we learn the routes that got configured on the rootns - they have two 
        static routes. </p>
	<p> </p>

<pre>


    [root@rhel8b ~]# ip route show
    default via 192.168.122.1 dev enp1s0 proto dhcp metric 100
    172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
    172.20.0.0/16 dev veth0 proto kernel scope link src 172.20.0.5
    172.20.0.0/16 dev veth1 proto kernel scope link src 172.20.0.6
    192.168.122.0/24 dev enp1s0 proto kernel scope link src 192.168.122.130 metric 100
    
    There are two static routes of interest here , this means any packet originating from the rootns heading toward the subnet 172.20.0.0/16 will be going out of veth0  interface with an IP address of 172.20.0.5.
    
    172.20.0.0/16 dev veth0 proto kernel scope link src 172.20.0.5
    172.20.0.0/16 dev veth1 proto kernel scope link src 172.20.0.6
    
    Since 172.20.0.6 falls within the subnet range of 172.20.0.0/16 , packet gets routed through this interface hitting the cnetns0 and will reach back to the cnetns1.
    
    Additional way to solidify the theory is to look at the routing cache, which indicates the same that in order to get to 172.20.0.60, the packet goes through the veth0.
    
    [root@rhel8b ~]# ip route get 172.20.0.60
    172.20.0.60 dev veth0 src 172.20.0.5 uid 0
        cache
    
    For additional reference, did tcpdumps on the cnetns0 interface while the ping was fired from the rootns and there was ARP calls indicating the packet reached this interface but unable to find out where to route the packets to.
    
    
    [root@rhel8b-cnetns0 ~ # tcpdump -nni ceth0 -vvv
    dropped privs to tcpdump
    tcpdump: listening on ceth0, link-type EN10MB (Ethernet), capture size 262144 bytes
    21:09:53.808981 ARP, Ethernet (len 6), IPv4 (len 4), Request who-has 172.20.0.60 tell 172.20.0.5, length 28
    
    </pre>		

</div>
<!-- container -->		


<!-- container -->		
<div class="screen-one-item-container">
	<h3>  Configure the bridge. </h3>
	<p> The static routes were added when the interface was brought up with valid ip’s. So if one of these routes were to be deleted, it will break the communication to the other network namespace!  It’s natural to have multiple network namespaces ( docker containers) running in one VM - so this is a valid use case that must work!

        172.20.0.0/16 dev veth0 proto kernel scope link src 172.20.0.5
        172.20.0.0/16 dev veth1 proto kernel scope link src 172.20.0.6 
        
        So the solution here is to create a bridge and attach the interfaces to it. Bridge acts on the L2 layer, this means it does the switching but not the routing but any interfaces using the bridge would have it’s MAC address added and on the L2 layer the packets can be 
        
        
         </p>
	<p> </p>

<pre>

     
    Delete the IP addresses associated with the veth0 and veth1
    
    [root@rhel8b ~]# ip addr delete 172.20.0.5/16 dev veth0
    [root@rhel8b ~]# ip addr delete 172.20.0.6/16 dev veth1
    
    Flusing these out will also remove the routes that were added.
    
    [root@rhel8b ~]# ip route show
    default via 192.168.122.1 dev enp1s0 proto dhcp metric 100
    172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
    192.168.122.0/24 dev enp1s0 proto kernel scope link src 192.168.122.130 metric 100
    

    [root@rhel8b ~]# ip link add br00 type bridge
    [root@rhel8b ~]# ip link set br00 up
  
    [root@rhel8b ~]# ip link set veth0 master br00
    [root@rhel8b ~]# ip link set veth1 master br00
    
    [root@rhel8b ~]# ip link show br00
    8: br00: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
        link/ether 82:5c:78:97:0b:f4 brd ff:ff:ff:ff:ff:ff
    

    [root@rhel8b ~]# ip link show
    1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    2: enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000
        link/ether 52:54:00:01:1a:b3 brd ff:ff:ff:ff:ff:ff
    3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default
        link/ether 02:42:67:e9:9c:2a brd ff:ff:ff:ff:ff:ff
    5: veth0@if4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master br00 state UP mode DEFAULT group default qlen 1000
        link/ether 32:e4:7c:d5:bd:8f brd ff:ff:ff:ff:ff:ff link-netns cnetns0
    7: veth1@if6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master br00 state UP mode DEFAULT group default qlen 1000
        link/ether ea:c9:3d:5b:64:59 brd ff:ff:ff:ff:ff:ff link-netns cnetns1
    8: br00: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
        link/ether 32:e4:7c:d5:bd:8f brd ff:ff:ff:ff:ff:ff
    
    Checking the connectivity on the bridge level, the Mac addresses of both the interfaces are learned in each of these namespaces. 
    
    [root@rhel8b-cnetns0 ~ # ip neigh
    172.20.0.60 dev ceth0 lladdr f6:0e:f5:ee:d1:7e STALE
    
    [root@rhel8b-cnetns0 ~ # ip link show
    1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    4: ceth0@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
        link/ether 3a:9c:07:b7:10:bd brd ff:ff:ff:ff:ff:ff link-netnsid 0
    
    [root@rhel8b-cnetns1 ~ # ip neigh
    172.20.0.50 dev ceth1 lladdr 3a:9c:07:b7:10:bd STALE
    
    [root@rhel8b-cnetns1 ~ # ip link show
    1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    6: ceth1@if7: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
        link/ether f6:0e:f5:ee:d1:7e brd ff:ff:ff:ff:ff:ff link-netnsid 0
    
    
</pre>		

</div>
<!-- container -->		


<!-- container -->		
<div class="screen-one-item-container">
	<h3>  Ping tests failures between rootns/cnetns*  </h3>
	<p> At this point  since we have the bridge configured and MAC”s for each of these cethX interfaces are learned by the bridge the L2 
        Layer packet forwarding should work and rootns should be able to reach cnetns0 and cnetns1 and also between the centns* interfaces there should be reachability as well. On the contrary  - none of them are working.
        
        So I want to break this down into two issues
        
        a) Root NS cannot talk to cnetns*
        b) cnetns cannot talk to each other.
    </p>
	<p> </p>

<pre>
</pre>		

</div>
<!-- container -->		


<!-- container -->		
<div class="screen-one-item-container">
	<h3>  Why IP’s within cnetns are not reachable from root namespace? </h3>
	<p> Issue-1 : Root NS cannot to ceth0 and ceth1 on the child namespaces. Looking at the routes, any IP’s headed towards the subnet 172.17.0.0/16 will be going out of docker0 interface and any IP’s headed towards 192.168.122.0/24  will be going out of enp1s0 interface and rest all should go via enp1s0 as part of the default route “default via 192.168.122.1 dev enp1s0”.

        So this means a ping to 172.20.0.50 will never reach veth0 since it will be routed through the enp1s0 unless there’s a static route added here bound to the veth0 interface.
        </p>
	<p> </p>

<pre>

    [root@rhel8b ~]# ping -c1 172.20.0.50
    PING 172.20.0.50 (172.20.0.50) 56(84) bytes of data.
    
    --- 172.20.0.50 ping statistics ---
    1 packets transmitted, 0 received, 100% packet loss, time 0ms
    
    
    [root@rhel8b ~]# ip route show
    default via 192.168.122.1 dev enp1s0 proto dhcp metric 100
    172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
    192.168.122.0/24 dev enp1s0 proto kernel scope link src 192.168.122.130 metric 100
    
    Let me try a tcpdump to validate the theory (before the static route is added to see if the packets are hitting the wire on the enp1s0 ).
    
    
    [root@rhel8b ~]# tcpdump -nni enp1s0 net 172.20.0.0/16
    dropped privs to tcpdump
    tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
    listening on enp1s0, link-type EN10MB (Ethernet), capture size 262144 bytes
    14:06:12.876136 IP 192.168.122.130 > 172.20.0.50: ICMP echo request, id 6820, seq 1, length 64
    
    So by adding a gateway IP address to the bridge, it should also plug in a static route in the routing table to ensure anything that should be routed for this network 172.20.0.0/16 should be going via br00 bridge than the enp1s0. 
    
    
    
    [root@rhel8b ~]# ip addr show br00
    8: br00: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
        link/ether 32:e4:7c:d5:bd:8f brd ff:ff:ff:ff:ff:ff
        inet6 fe80::30e4:7cff:fed5:bd8f/64 scope link
           valid_lft forever preferred_lft forever
    [root@rhel8b ~]# ip addr add 172.20.0.1/16 dev br00
    [root@rhel8b ~]# ip addr show br00
    8: br00: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
        link/ether 32:e4:7c:d5:bd:8f brd ff:ff:ff:ff:ff:ff
        inet 172.20.0.1/16 scope global br00
           valid_lft forever preferred_lft forever
        inet6 fe80::30e4:7cff:fed5:bd8f/64 scope link
           valid_lft forever preferred_lft forever
    [root@rhel8b ~]# ip route show
    default via 192.168.122.1 dev enp1s0 proto dhcp metric 100
    172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
    172.20.0.0/16 dev br00 proto kernel scope link src 172.20.0.1
    192.168.122.0/24 dev enp1s0 proto kernel scope link src 192.168.122.130 metric 100
    
    
    Let’s try the pings again…
    
    [root@rhel8b ~]# ping -c1 172.20.0.50
    PING 172.20.0.50 (172.20.0.50) 56(84) bytes of data.
    64 bytes from 172.20.0.50: icmp_seq=1 ttl=64 time=0.089 ms
    
    --- 172.20.0.50 ping statistics ---
    1 packets transmitted, 1 received, 0% packet loss, time 0ms
    rtt min/avg/max/mdev = 0.089/0.089/0.089/0.000 ms
    
    So with the route in place , the rootns now can reach to the cnetns0 and cnetns1
    
    Issue still exists where the cnetns namespaces can’t talk to each other still..
    
    
    
</pre>		

</div>
<!-- container -->		


<!-- container -->		
<div class="screen-one-item-container">
	<h3>  HEADER  </h3>
	<p> </p>
	<p> </p>

<pre>
CODE BLOCK
</pre>		

</div>
<!-- container -->		



<!-- container -->		
<div class="screen-one-item-container">
	<h3>  HEADER  </h3>
	<p> </p>
	<p> </p>

<pre>
CODE BLOCK
</pre>		

</div>
<!-- container -->		



<!-- container -->		
<div class="screen-one-item-container">
	<h3>  HEADER  </h3>
	<p> </p>
	<p> </p>

<pre>
CODE BLOCK
</pre>		

</div>
<!-- container -->		


	

</body>
</html>
